dplyr::group_by(Family_ID) %>%
dplyr::summarise(n=n())
# number of samples per family in LN2, N = 4
baseline_samples %>%
dplyr::filter(Fixative %in% 'LN2') %>%
dplyr::group_by(Family_ID) %>%
dplyr::summarise(n=n())
# number of samples per family in DNA RNMA shield, N = 3
baseline_samples %>%
dplyr::filter(!Fixative %in% 'LN2') %>%
dplyr::group_by(Family_ID) %>%
dplyr::summarise(n=n())
samples_metadata.all <- samples.metadata %>%
dplyr::select('Tube.ID','Purpose','Fixative',
'Tank_number','Prime_treatment','Timepoint') %>%
dplyr::filter(Timepoint %in% c('post-prime',
'1 week recovery, post challenge'
))
samples_metadata.all <- samples_metadata %>%
dplyr::select('Tube.ID','Purpose','Fixative',
'Tank_number','Prime_treatment','Timepoint') %>%
dplyr::filter(Timepoint %in% c('post-prime',
'1 week recovery, post challenge'
))
samples.metadata.LN2 <- samples_metadata.all %>% filter(Fixative %in% 'LN2')
samples.metadata.LN2 %>%
dplyr::group_by(Prime_treatment, Timepoint) %>%
dplyr::summarise(n=n()) %>%
dplyr::arrange(Timepoint)
samples.metadata.LN2 %>%
dplyr::group_by(Prime_treatment, Timepoint,Tank_number) %>%
dplyr::summarise(n=n()) %>%
dplyr::arrange(Timepoint)
# number of samples per family in LN2, N = 4
baseline_samples %>%
dplyr::filter(Fixative %in% 'LN2') %>%
dplyr::group_by(Family_ID) %>%
dplyr::summarise(n=n())
baseline_samples %>%
dplyr::filter(Fixative %in% 'LN2')
samples_metadata.all
samples.metadata.LN2
samples.metadata.LN2 %>%
dplyr::group_by(Prime_treatment, Timepoint,Tank_number) %>%
dplyr::summarise(n=n()) %>%
dplyr::arrange(Timepoint)
nrow(samples)
View(samples)
samples_metadata %>% dplyr::group_by(Timepoint, Fixative) %>%
summarise(n = n())
samples_metadata %>% dplyr::group_by(Timepoint, Purpose) %>%
summarise(n = n())
unique(samples_metadata$Tank_number)
knitr::opts_chunk$set(echo = TRUE)
# SET WORKING DIRECTORY
knitr::opts_knit$set(root.dir = "C:/Users/gurrs/Documents/Github_repositories/Cgigas_temperature_prime/RAnalysis") # Sam's work
library(tidyverse)
# load the raw data, just the counts of dead individuals each day
metadata <- read.csv("Data/experiment_metadata.csv", header = T, sep =',')
samples  <- read.csv("Data/sample_spreadsheet.csv", header = T, sep =',')
baseline_samples <- samples %>% dplyr::filter(Timepoint %in% 'baseline')
samples_metadata <- merge(metadata,
(samples %>% dplyr::filter(!Timepoint %in% 'baseline')),
by = 'Tank_number')
nrow(samples_metadata) == (nrow(samples) - nrow(baseline_samples))
View(samples)
samples_metadata %>% dplyr::group_by(Timepoint, Purpose) %>%
summarise(n = n())
# number of samples per family in LN2, N = 4
baseline_samples %>%
dplyr::filter(Fixative %in% 'LN2') %>%
dplyr::group_by(Family_ID) %>%
dplyr::summarise(n=n())
# number of samples per family in DNA RNMA shield, N = 3
baseline_samples %>%
dplyr::filter(!Fixative %in% 'LN2') %>%
dplyr::group_by(Family_ID) %>%
dplyr::summarise(n=n())
# number of samples per family in LN2, N = 4
baseline_samples %>%
dplyr::filter(Fixative %in% 'LN2') %>%
dplyr::group_by(Family_ID) %>%
dplyr::summarise(n=n())
samples_metadata.all <- samples_metadata %>%
dplyr::select('Tube.ID','Purpose','Fixative',
'Tank_number','Prime_treatment','Timepoint') %>%
dplyr::filter(Timepoint %in% c('post-prime',
'1 week recovery, post challenge'
))
samples.metadata.LN2 <- samples_metadata.all %>% filter(Fixative %in% 'LN2')
samples.metadata.LN2 %>%
dplyr::group_by(Prime_treatment, Timepoint,Tank_number) %>%
dplyr::summarise(n=n()) %>%
dplyr::arrange(Timepoint)
DNARNA.samples.metadata <- All_samples.metadata %>% filter(!Fixative %in% 'LN2')
p
samples_metadata.all <- samples_metadata %>%
dplyr::select('Tube.ID','Purpose','Fixative',
'Tank_number','Prime_treatment','Timepoint') %>%
dplyr::filter(Timepoint %in% c('post-prime',
'1 week recovery, post challenge'
))
samples.metadata.LN2 <- samples_metadata.all %>% filter(Fixative %in% 'LN2')
samples.metadata.LN2
samples.16S <- samples.metadata.LN2 %>%
dplyr::filter(!Prime_treatment %in% (6_hours))
samples.16S <- samples.metadata.LN2 %>%
dplyr::filter(!Prime_treatment %in% "6_hours")
samples.16S
samples.16S <- samples.metadata.LN2 %>%
dplyr::filter(!Prime_treatment %in% "6_hours") %>%
dplyr::arrange(desc(Tube.ID))
samples.16S
samples.16S <- samples.metadata.LN2 %>%
dplyr::filter(!Prime_treatment %in% "6_hours") %>%
dplyr::arrange(Tube.ID)
View(samples.16S)
samples_metadata.all <- samples_metadata %>%
dplyr::select('Tube.ID','Purpose','Fixative',
'Tank_number','Prime_treatment','Timepoint') %>%
dplyr::filter(Timepoint %in% c('post-prime',
'1 week recovery, post challenge',
'3 week recovery post challenge'
))
samples.metadata.LN2 <- samples_metadata.all %>% filter(Fixative %in% 'LN2')
samples.metadata.LN2
baseline_samples
samples_metadata
samples.16S <- samples.metadata.LN2 %>%
dplyr::filter(!Prime_treatment %in% "6_hours") %>%
dplyr::arrange(Tube.ID)
View(samples.16S)
samples_metadata.all <- samples_metadata %>%
dplyr::select('Tube.ID','Purpose','Fixative',
'Tank_number','Prime_treatment','Timepoint') %>%
dplyr::filter(Timepoint %in% c('post-prime',
'1 week recovery, post challenge',
'3 week recovery post challenge'
))
samples.metadata.LN2 <- samples_metadata.all %>% filter(Fixative %in% 'LN2')
samples.16S <- samples.metadata.LN2 %>%
dplyr::filter(!Prime_treatment %in% "6_hours") %>%
dplyr::arrange(Tube.ID)
View(samples.16S)
samples_metadata
samples_metadata.all <- samples_metadata %>%
dplyr::select('Tube.ID','Purpose','Fixative',
'Tank_number','Prime_treatment','Timepoint') %>%
dplyr::filter(Timepoint %in% c('post-prime',
'1 week recovery, post challenge',
'3 week recovery, post challenge'
))
samples.metadata.LN2 <- samples_metadata.all %>% filter(Fixative %in% 'LN2')
samples.16S <- samples.metadata.LN2 %>%
dplyr::filter(!Prime_treatment %in% "6_hours") %>%
dplyr::arrange(Tube.ID)
View(samples.16S)
samples_metadata.all
samples_metadata
samples_metadata.all <- samples_metadata %>%
dplyr::select('Tube.ID','Purpose','Fixative',
'Tank_number','Prime_treatment','Timepoint') %>%
dplyr::filter(Timepoint %in% c('post-prime',
'1 week recovery, post challenge',
'3 weeks recovery, post challenge'
))
samples.metadata.LN2 <- samples_metadata.all %>% filter(Fixative %in% 'LN2')
samples.16S <- samples.metadata.LN2 %>%
dplyr::filter(!Prime_treatment %in% "6_hours") %>%
dplyr::arrange(Tube.ID)
View(samples.16S)
metadata
samples_metadata <- merge(metadata,
(samples %>% dplyr::filter(!Timepoint %in% 'baseline')),
by = 'Tank_number')
samples_metadata
samples_metadata
# number of samples per family in LN2, N = 4
baseline_samples %>%
dplyr::filter(Fixative %in% 'LN2') %>%
dplyr::group_by(Family_ID) %>%
dplyr::summarise(n=n())
# number of samples per family in DNA RNMA shield, N = 3
baseline_samples %>%
dplyr::filter(!Fixative %in% 'LN2') %>%
dplyr::group_by(Family_ID) %>%
dplyr::summarise(n=n())
samples_metadata.all <- samples_metadata %>%
dplyr::select('Tube.ID','Purpose','Fixative',
'Tank_number','Prime_treatment','Timepoint') %>%
dplyr::filter(Timepoint %in% c('post-prime',
'1 week recovery, post challenge',
'3 weeks recovery, post challenge'
))
samples_metadata.all
baseline_samples
samples_metadata
baseline_samples
samples_metadata
baseline_samples
samples_metadata
samples_metadata.all <- samples_metadata %>%
dplyr::select('Tube.ID','Purpose','Fixative',
'Tank_number','Prime_treatment','Timepoint') %>%
dplyr::filter(Timepoint %in% c('post-prime',
'1 week recovery, post challenge',
'3 weeks recovery, post challenge'
))
samples_metadata.all
baseline_samples
samples.16S
samples.16S <- samples.metadata.LN2 %>%
dplyr::filter(!Prime_treatment %in% "6_hours") %>%
dplyr::arrange(Tube.ID) %>%
dplyr::select(!Purpose)
View(samples.16S)
baseline_samples
baseline_samples.16S <- baseline_samples %>% dplyr::filter(Fixative %in% "LN2")
samples.16S
baseline_samples.16S <- baseline_samples %>%
dplyr::filter(Fixative %in% "LN2") %>%
dplyr::select(Tube.ID, Family, Timepoint, Prime_treatment, Tank_number, Fixative)
baseline_samples
baseline_samples.16S <- baseline_samples %>%
dplyr::filter(Fixative %in% "LN2") %>%
dplyr::mutate(Prime_treatment = NA) %>%
dplyr::select(Tube.ID, Family_ID, Timepoint, Prime_treatment, Tank_number, Fixative)
baseline_samples.16S <- baseline_samples %>%
dplyr::filter(Fixative %in% "LN2") %>%
dplyr::mutate(Prime_treatment = NA) %>%
dplyr::select(Tube.ID, Family_ID, Timepoint, Prime_treatment, Tank_number, Fixative)
samples.16S <- samples.metadata.LN2 %>%
dplyr::filter(!Prime_treatment %in% "6_hours") %>%
dplyr::arrange(Tube.ID) %>%
dplyr::select(!Purpose) %>%
dplyr::mutate(Prime_treatment = NA) %>%
dplyr::select(Tube.ID, Family_ID, Timepoint, Prime_treatment, Tank_number, Fixative)
samples.16S <- samples.metadata.LN2 %>%
dplyr::filter(!Prime_treatment %in% "6_hours") %>%
dplyr::arrange(Tube.ID) %>%
dplyr::select(!Purpose) %>%
dplyr::mutate(Family_ID = NA) %>%
dplyr::select(Tube.ID, Family_ID, Timepoint, Prime_treatment, Tank_number, Fixative)
rbind(baseline_samples.16S, samples.16S
rbind(baseline_samples.16S, samples.16S)
rbind(baseline_samples.16S, samples.16S)
all.16s <- rbind(baseline_samples.16S, samples.16S)
all.16s
View(samples.16S)
View(all.16s)
write.csv(all.16s, "Output/samples_16S_sequencing.csv")
knitr::opts_chunk$set(echo = TRUE)
# SET WORKING DIRECTORY
knitr::opts_knit$set(root.dir = "C:/Users/gurrs/Documents/Github_repositories/Cgigas_temperature_prime/RAnalysis") # Sam's work
write.csv(all.16s, "Output/samples_16S_sequencing.csv")
write.csv(all.16s, "Data/Sequencing/16S_samples.csv")
samples.16S
require(lubridate)
require(dplyr)
library(readr)
Thresholds<-c(.99,1.99,2.99,3.49,4.79); #Choose any number of thresholds for hypoxia (DO<=Threshold)
ThresholdLabels<-c("Severe","Moderately Severe", "Moderate","Marginal", "Sub-Optimal"); #Choose labels for these thresholds
MinEvent<-59.9; #defined in minutes, the minimum duration of hypoxia to be considered an event
#(slightly shorter than interval because exactly 1 hr events were being discarded for some reason.)
Interval<-60; #defined in minutes, the maximum distance (<=) between two measured hypoxic points that are part of same event
timeDigits<-5; #specifies the number of decimal places to which the time data are accurate.
# CHANGE YOUR WORKING DIRECTORY TO THE SONDE DATA FOLDER
getwd()
setwd("C:/Users/gurrs/Documents/Documents/3_Projects/2026_YaquinaProjec/")
filename <- as.character(file.choose())
D        <- readLines(filename)
ind      <- grep('Date Time',D)
D
D        <- readLines(filename)
ind      <- grep('time',D)
ind
D
ind      <- grep('UTC',D)
ind
raw      <- read_csv(filename,skip=ind-1,col_types = cols()) #header=TRUE, sep = ","));
raw
columns  <- names(raw)
columns
raw_df   <- as.data.frame(raw) %>% dplyr::select('UTC', 'mg/l')
raw_df
data <- as.data.frame(raw_df[!is.na(UTC),] %>%
dplyr::mutate(TIME = as_datetime(UTC), # convert time and rename
# NOTE: lubraidate as numeric converts to number of seconds since 1/1/1970 - convert this to number of years
TIME_NUM_FORMAT = (as.numeric(TIME) / 86400 / 365) ) %>% # get a numeric versoin of time
dplyr::select(TIME, TIME_NUM_FORMAT, `RDO Concentration (mg/L)`) %>%  # call the three column of interest
dplyr::rename(DO  = `RDO Concentration (mg/L)`)) %>%
na.omit
raw_df
as.data.frame(raw_df[!is.na(UTC),] %>%
dplyr::mutate(TIME = as_datetime(UTC), # convert time and rename
# NOTE: lubraidate as numeric converts to number of seconds since 1/1/1970 - convert this to number of years
TIME_NUM_FORMAT = (as.numeric(TIME) / 86400 / 365) )
raw_df[!is.na(UTC),] %>%
dplyr::mutate(TIME = as_datetime(UTC), # convert time and rename
# NOTE: lubraidate as numeric converts to number of seconds since 1/1/1970 - convert this to number of years
TIME_NUM_FORMAT = (as.numeric(TIME) / 86400 / 365) )
raw_df %>%
dplyr::mutate(TIME = as_datetime(UTC), # convert time and rename
# NOTE: lubraidate as numeric converts to number of seconds since 1/1/1970 - convert this to number of years
TIME_NUM_FORMAT = (as.numeric(TIME) / 86400 / 365) )
data <- raw_df %>%
dplyr::mutate(TIME = as_datetime(UTC), # convert time and rename
# NOTE: lubraidate as numeric converts to number of seconds since 1/1/1970 - convert this to number of years
TIME_NUM_FORMAT = (as.numeric(TIME) / 86400 / 365) ) %>% # get a numeric versoin of time
dplyr::select(TIME, TIME_NUM_FORMAT, `RDO Concentration (mg/L)`) %>%  # call the three column of interest
dplyr::rename(DO  = `mg/l`) %>%
na.omit
data <- raw_df %>%
dplyr::mutate(TIME = as_datetime(UTC), # convert time and rename
# NOTE: lubraidate as numeric converts to number of seconds since 1/1/1970 - convert this to number of years
TIME_NUM_FORMAT = (as.numeric(TIME) / 86400 / 365) ) %>% # get a numeric versoin of time
dplyr::select(TIME, TIME_NUM_FORMAT, `mg/l`) %>%  # call the three column of interest
dplyr::rename(DO  = `mg/l`) %>%
na.omit
data
#Sorts the data by time in ascending order before anything else happens.
data<-data[order(data[[2]]),];
#Separates out the elements into their own variables for easy use.
timeString<-as.character(data[[1]]);
timeNum<-data[[2]];
DO<-(data[[3]]);
#Creates a character vector to store what will eventually be the report.
dataReport<-character();
#Creates variables(s) that will eventually be output to (a) file(s) (hopefully if I get this right).
type<-character();
startTime<-character();
endTime<-character();
#Sorts the data by time in ascending order before anything else happens.
data<-data[order(data[[2]]),];
#Separates out the elements into their own variables for easy use.
timeString<-as.character(data[[1]]);
timeNum<-data[[2]];
DO<-(data[[3]]);
#Creates a character vector to store what will eventually be the report.
dataReport<-character();
#Creates variables(s) that will eventually be output to (a) file(s) (hopefully if I get this right).
type<-character();
startTime<-character();
endTime<-character();
duration<-numeric();
eventDO<-numeric();
startNum<-numeric();
endNum<-numeric();
avgAvgDO<-numeric();
avgDur<-numeric();
# every hour is 0.000115
# 0.0001141553*24*365 -- 0.0001141553 == 1 hour to years
#Defines the minimum event and maximum distance of measurements in terms of time code (new unit=days).
minEventNum<-round(MinEvent/24/60/365,digits=timeDigits);#Rounded to the same level of precision as the time data
intervalNum<-round(Interval/24/60/365,digits=timeDigits);#Rounded to the same level of precision as the time data
#Here's where the analysis begins in earnest:
#Defines and initializes variables to hold the starting and ending row numbers of each run detected
runStart<-0;#must start at 0 in order to work.
runEnd<-0;#must start at 0 in order to work.
#Defines a vector for keeping track of the number of runs of each type.
runCounts<-numeric();
#Loop through for each threshold.
for(j in 1:length(Thresholds)){
#Defines and initializes a variable for counting runs.
NumberOfRuns<-0;
#Defines and vector for keeping track of DOs.
AvgDOs<-numeric();
#defines a vector to keep track of durations
durs<-numeric();
#The loop that iterates through each row of the data and finds runs:
for (i in 1:length(timeNum)){
#if we're not currently in a run...
if(runEnd==0){
#if we've found a starting point, set the start and end of the run to that time.
if(DO[i]<=Thresholds[j]){
runStart<-i;
runEnd<-i;
};
}#LINE TERMINATOR WILL MESS THIS UP
#if we're already in a run...
else{
#if we haven't exceeded the maximum allowable gap between hypoxic points...
if(round(timeNum[i]-timeNum[runEnd],digits=timeDigits)<=intervalNum){
#and we've just found another applicable point, reset the end point to this new point.
if(DO[i]<=Thresholds[j]){
runEnd<-i;
};
#If we hit the end of the file during a run that meets our criteria.
if(i==length(timeNum)&&(round(timeNum[runEnd]-timeNum[runStart],digits=timeDigits)>=minEventNum)){
#DO SHIT HERE FOR ANALYSIS
NumberOfRuns<-NumberOfRuns+1;
avgDO<-mean(DO[runStart:runEnd]);
avgDOround<-round(avgDO,digits=2);
dataReport[length(dataReport)+1]<-(paste("Found",ThresholdLabels[j],"Event #",NumberOfRuns," Avg DO:",
avgDOround," Start/End:",timeString[runStart],"-",timeString[runEnd],"(Cut Off By End Of File)"));
#for the output reports
startTime[length(startTime)+1]<-timeString[runStart];#stores the start time
endTime[length(endTime)+1]<-NA;#stores the end time
startNum[length(startNum)+1]<-round(timeNum[runStart],digits=timeDigits);#stores the start num
endNum[length(endNum)+1]<-NA;#stores the end num
#Actually we don't want to update DO for incomplete events.
#AvgDOs[length(AvgDOs)+1]<-avgDO; #stores the avg DO for each event
duration[length(duration)+1]<-NA;
eventDO[length(eventDO)+1]<-NA;
type[length(type)+1]<-ThresholdLabels[j];
#Actually we don't want to update durations for averaging because this doesn't count
#durs[length(durs)+1]<-(timeNum[runEnd]-timeNum[runStart])*24*60; #stores event durations
#set these back to 0 for the next file, or else shit will go badly if I try to automate.
runStart<-0;
runEnd<-0;
};
}#LINE TERMINANTOR WILL MESS THIS UP
#if we've exceeded the maximum allowable gap and not found an applicable point...
else{
#if the run we've found is at least the minimum length we've defined...
if(round(timeNum[runEnd]-timeNum[runStart],digits=timeDigits)>=minEventNum){
#DO SHIT HERE FOR ANALYSIS
NumberOfRuns<-NumberOfRuns+1;
avgDO<-mean(DO[runStart:runEnd]);
avgDOround<-round(avgDO,digits=2);
#Contingency for if run starts at beginning of file
if(runStart==1){dataReport[length(dataReport)+1]<-(
paste("Found",ThresholdLabels[j],"Event #",NumberOfRuns," Avg DO:",
avgDOround," Start/End:",timeString[runStart],"(Cut off by beginning of file)","-",timeString[runEnd]));
#for the output reports
startTime[length(startTime)+1]<-NA;#stores the start time
endTime[length(endTime)+1]<-timeString[runEnd];#stores the end time
startNum[length(startNum)+1]<-NA;#stores the start num
endNum[length(endNum)+1]<-round(timeNum[runEnd],digits=timeDigits);#stores the end num
#we don't want to store this in this case.
#AvgDOs[length(AvgDOs)+1]<-avgDO; #stores the avg DO for each event
duration[length(duration)+1]<-NA;
eventDO[length(eventDO)+1]<-NA;
type[length(type)+1]<-ThresholdLabels[j];
#we don't want to store this in this case.
#durs[length(durs)+1]<-(timeNum[runEnd]-timeNum[runStart])*24*60; #stores event durations
}
#Every other case
else{dataReport[length(dataReport)+1]<-(paste("Found",ThresholdLabels[j],"Event #",NumberOfRuns," Avg DO:",
avgDOround," Start/End:",timeString[runStart],"-",timeString[runEnd]));
#for the output reports
startTime[length(startTime)+1]<-timeString[runStart];#stores the start time
endTime[length(endTime)+1]<-timeString[runEnd];#stores the end time
startNum[length(startNum)+1]<-round(timeNum[runStart],digits=timeDigits);#stores the start num
endNum[length(endNum)+1]<-round(timeNum[runEnd],digits=timeDigits);#stores the end num
AvgDOs[length(AvgDOs)+1]<-avgDO; #stores the avg DO for each event
duration[length(duration)+1]<-(timeNum[runEnd]-timeNum[runStart])*24*60*365;
eventDO[length(eventDO)+1]<-avgDO;
type[length(type)+1]<-ThresholdLabels[j];
durs[length(durs)+1]<-(timeNum[runEnd]-timeNum[runStart])*24*60*365; #stores event durations
}
};
#regardless of whether we ran analysis or not, set these varaibles to 0 so the loop
#knows we're not currently in a run anymore.
runStart<-0;
runEnd<-0;
};
};
};
runCounts[j]<-NumberOfRuns; #stores the total events for this threshold detected.
dataReport[length(dataReport)+1]<-"";#adds a blank line to the data file to separate blocks.
avgAvgDO[j]<-mean(AvgDOs); #updates the average average DO for the second report file.
avgDur[j]=mean(durs);#updates the average average duration for the second report file.
};
#adds the summary lines to the data report.
dataReport[length(dataReport)+1]<-"Events Detected:";
for(k in 1:length(Thresholds)){
dataReport[length(dataReport)+1]<-paste(ThresholdLabels[k],"Events:",runCounts[k]);
};
#compiles the reports for saving
report1<-data.frame(
type=type,
durationMinutes=duration,
startTime=startTime,
endTime=endTime,
startNum=startNum,
endNum=endNum,
eventDO=eventDO);
report1$type<-as.character(report1$type);
report1$startTime<-as.character(report1$startTime);
report1$endTime<-as.character(report1$endTime);
report2<-data.frame(
type=ThresholdLabels,
threshold=Thresholds,
numberFound=runCounts,
avgDurMinutes=avgDur,
avgAvgDO=avgAvgDO);
report2$type<-as.character(report2$type);
#saves out the report files.
output1<-gsub(".csv","_HypoxiaEvents.csv",filename);
write.csv(report1, file=output1,row.names=FALSE);
output2<-gsub(".csv","_HypoxiaSummary.csv",filename);
write.csv(report2, file=output2,row.names=FALSE);
#prints the report.
print(dataReport);
# CHANGE YOUR WORKING DIRECTORY TO THE SONDE DATA FOLDER
getwd()
setwd("C:/Users/gurrs/Documents/Documents/3_Projects/2026_YaquinaProjec/")
# CHANGE YOUR WORKING DIRECTORY TO THE SONDE DATA FOLDER
getwd()
require(lubridate)
require(dplyr)
library(readr)
Thresholds<-c(.99,1.99,2.99,3.49,4.79); #Choose any number of thresholds for hypoxia (DO<=Threshold)
ThresholdLabels<-c("Severe","Moderately Severe", "Moderate","Marginal", "Sub-Optimal"); #Choose labels for these thresholds
MinEvent<-59.9; #defined in minutes, the minimum duration of hypoxia to be considered an event
#(slightly shorter than interval because exactly 1 hr events were being discarded for some reason.)
Interval<-60; #defined in minutes, the maximum distance (<=) between two measured hypoxic points that are part of same event
timeDigits<-5; #specifies the number of decimal places to which the time data are accurate.
# CHANGE YOUR WORKING DIRECTORY TO THE SONDE DATA FOLDER
getwd()
setwd("C:/Users/gurrs/Documents/Documents/3_Projects/2026_YaquinaProjec/")
